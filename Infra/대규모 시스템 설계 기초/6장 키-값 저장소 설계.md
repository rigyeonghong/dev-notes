* 키-값 저장소 : 키-값 데이터베이스라고 불리는 비관계형 데이터베이스
* 키 : 고유 식별자. 일반 텍스트/해시 값
* 값 : 문자열, 리스트, 객체
* ex. 아마존 다이나모, memcached, 레디스

# 키-값 저장소 설계시 문제 이해 및 설계 범위 확정
* 완벽한 설계 없음
* 읽기, 쓰기, 메모리 사용량 사이에 균형 & 데이터 일관성과 가용성 사이 타협 필요

# 단일 서버 키-값 저장소
* 직관적) 키-값 쌍 전부 메모리에 해시 테이블로 저장
	* 장점) 빠른 속도
	* 단점) 모든 데이터를 메모리 안에 두는 것이 불가
* 해결) 데이터 압축, 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장

그러나 한대 서버로 부족한 때 찾아오면,
# 분산 키-값 저장소
* 분산 키-값 저장소 = 분산 해시 테이블 : 키-값 쌍을 여러 서버에 분산시키는 탓
#### CAP 정리
아래 3가지 요구사항 동시 만족하는 분산시스템 설계 불가하다는 정리
* 데이터 일관성
	* 모든 클라이언트는 어떤 노드에 접속했냐 관계없이 언제나 같은 데이터 봄
* 가용성
	* 일부 노드 장애 발생해도 항상 응답 받아야함
* 파티션 감내
	* 두 노드 사이 통신 장애 발생시, 네트워크 파티션이 생겨도 시스템 계속 동작해야함
![[최근 사진 보기.jpeg]]\
* CA : 존재하지 않음. 통상 nw 장애는 피할 수 없는 일로 여겨지므로, 분산 시스템은 반드시 파티션 문제를 감내하도록 설계되야함

예시) n1에 기록된 데이터가 자동으로 n2, n3에 복제되는 환경에서, n3 장애 발생시 일관성, 가용성 사이 선택 필요
![[IMG_42BBDF69DBA9-1.jpeg]]
➡️ 일관성 선택
	불일치 문제 피하기위해 n1,n2 에 대해 쓰기 연산 중단시키면 가용성 깨짐.
	ex. 은행권 시스템 

➡️ 가용성
	낡은 데이터를 반환할 위험이 있더라도 계속 읽기 연산 허용.
	n1,n2 쓰기 연산 허용
	파티션 문제 해결 후 새 데이터 n3서버에 전송

# 시스템 컴포넌트
* 키-값 저장소 구현에 사용될 핵심 컴포넌트 및 기술 
* ex. 다이나모, 카산드라, 빅테이블 사례 참조 

### 1. 데이터 파티션
* 대규모 애플리케이션은 전체 데이터 한대 서버에 욱여넣는게 불가 
	➡️  데이터를 작은 파티션들로 분할 후 여러대 서버로 저장.

* 데이터 파티션으로 나눌때 고려점 ➡️ 안정 해시로 해결
	* 데이터를 여러 서버에 고르게 분산 가능한가
	* 노드가 추가/삭제 될 때 데이터 이동 최소화할 수 있나
	➡️ 안정 해시 사용시 장점
	* 규모 확장 자동화 : 부하에 따라 서버 자동 추가/삭제되도록 만들 수 있음
	* 다양성 : 각 서버 용량에 맞게 가상 노드 수 조정 가능

## 2. 데이터 다중화
* 높은 가용성과 안정성 확보 (정전,자연재해)위해 데이터를 N개 서버에 비동기적으로 다중화할 필요 존재.
	* N개 서버 선정법 
		키를 해시 링 위에 배치 후, 시계 방향으로 순회하며 만나는 첫 N개 서버에 데이터 사본 보관
	* 가상 노드 사용시, 실물리서버 개수가 n보다 작아질 수 있으므로 노드 선택시 같은 물리서버 중복 선택 불가하게 설정

## 3. 데이터 일관성
* 여러 노드에 다중화된 데이터는 적절히 동기화 필요 
	➡️ 정족수 합의 프로토콜 사용하면 읽기/쓰기 일관성 보장 가능

* `정족수(quorum)` : 네트워크 내의 노드 그룹에서 의사결정을 위해 필요한 최소한의 노드 수를 의미
* `N` : 사본 개수
* `W` : 쓰기 연산에 대한 정족수. 성공한 것으로 간주되려면 적어도 W 서버로부터 쓰기 연산 성공 응답 받아야함.
* `R` : 읽기 연산에 대한 정족수. 

* 중재자 : 클라이언트와 노드 사이 proxy 역할
![[대규모시스템설계기초.jpeg]]
* w=1 : 쓰기 연산 성공 판단위해 중재자는 최소 1대 서버로부터 쓰기 성공 응답 받아야함.

* W, R, N 값 정하는 것은 응답 지연, 데이터 일관성 사이의 타협점 찾는 과정.
	* ex. W=1, R=1 : 응답 속도 빠름
		 W=2, R=2 : 데이터 일관성 수준 향상, 응답 속도 느림
	* W+R > N : 강한 일관성

#### 면접시 N,W,R 정하는 법
* W=N, R=1 : 빠른 읽기 연산에 최적화된 시스템
* W=1, R=N : 빠른 쓰기 연산
* W+R > N : 강한 일관성
* W+R <= N : 강한 일관성 X 

## 4. 일관성 모델
* 데이터 일관성 수준 결정
* 종류
	* 강한 일관성
		모든  읽기 연산은 가장 최근 갱신된 결과 반환.
		➡️ 방법) 현재 W 반영시까지 해당 데이터 R/W 금지
		ex. 고가용성과 맞지 않음  
	* 약한 일관성
		읽기 연산은 가장 최근 갱신된 결과 반환 받지 못할 수 있음
	* 최종 일관성
		갱신 결과가 결국 모든 사본에 반영(동기화)
		➡️ 방법) W 병렬 발생시 일관성 깨질 수 있으나, 클라이언트가 문제 해결.
		  클라이언트측 데이터 버전 정보 활용해 일관성 깨진 데이터 읽지 않도록
		ex. 다이나모, 카산드라

## 5. 비 일관성 해소 기법 : 데이터 버저닝
* 버저닝과 벡터 시계로 문제 해결

 * 버저닝 
	 데이터 변경시마다 해당 데이터 새로운 버전 만드는 것.
	 각 버전의 데이터는 변경 불가능

* ex. 충돌 예시
![[Pasted image 20240518120618.png]]

* 벡터 시계
	[서버, 버전] 순서쌍을 데이터에 매단 것
	어떤 버전이 선행/후행 버전인지, 다른 버전과 충돌 판별하는데 쓰임
	D( [S1, v1] , [S2, v2] , , [Sn, vn] ) 
	* D : 데이터
	* vi : 버전 카운터
	* Si : 서버 번호

	* 데이터D를 서버 Si에 기록시 아래 작업 중 하나 수행
		* [Si, vi] 존재하면 vi 증가
		* 그렇지 않으면 [Si, 1] 생성

	![[Pasted image 20240518123549.png]]
	* 5. 어떤 클라이언트가 D3,D4 값을 읽으면 데이터 간 충돌 알게됨. 클라이언트가 해소 후 서버에 기록.	
	* Q 충돌을 어떻게 감지하는가?
		➡️ 버전별 구성 요소가 작은 값을 갖는 것이 있는지 확인

* 벡터 시계 사용 충돌 감지 해소의 단점
	* 충돌 감지, 해소 로직으로 클라이언트 구현 복잡
	* [서버:버전] 순서쌍 개수가 빨리 늘어남. ➡️ 임계치 설정 후 오랜 순서쌍부터 벡터시계에서 제거하도록.  

## 6. 장애 처리
* 대규모 시스템 장애 흔하므로 어떻게 처리할 것인지가 중요

#### 장애 감지
* 분산 시스템 보통 2대 이상 서버가 특정 서버의 장애를 보고해야 해당 서버가 실제로 장애 발생했다 간주

* 모든 노드 사이 멀티캐스팅 : 서버 장애 감지에 쉬운 방법이나 비효율
	![[Pasted image 20240518124816.png]]

 * 가십 프로토콜 : 분산형 장애 감지 솔루션 채택이 효율적
	![[Pasted image 20240518125355.png]]
	* 각 노드별 맴버십 목록(멤버ID, 박동 카운터쌍의 목록) 유지.
	* 각 노드 주기적으로 자신의 박동 카운터 증가.
	* 각 노드는 무작위로 선정된 노드에게 주기적 자기 박동 카운터 목록 전달
	* 전달 받은 노드는 멤버십 목록 갱신
	* 어떤 맴버(S2) 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 장애로 간주
	* 해당 맴버(S2) 를 포함한 목록을 무작위 선택 다른 노드에게 전달
	* S2 장애를 발견한 모든 노드는 해당 노드를 장애 노드로 표시

#### 일시적 장애 처리
* 가용성 보장 위해 필요한 조치
	* 엄격한 정족수 접근법(데이터 일관성) 사용시
		읽기,쓰기 금지
	* 느슨한 정족수 접근법(가용성 증가) 사용시
		쓰기 연산 수행할 W개의 건강한 서버와 읽기 연산 수행할 R개의 건강한 서버를 해시링에서 선택.
		nw 문제로 장애서버로 가는 요청 다른 서버가 잠시 맡아 처리. (임시 쓰기 연산 처리 서버에는 단서 남겨둠 )
		그간 발생한 변경사항은 해당 서버 복구시 일괄 반영. 

#### 영구 장애 처리
* 반-엔트로피 프로토콜 
	 * 머클(Merkle) 트리 사용 
		 * 목적 :  사본간의 일관성이 망가진 상태 탐지 ➡️ 전송 데이터의 양을 줄이기 위해. 
			 대규모 자료구조의 내용을 효과적이면서도 보안상 안전한 방법으로 검증 가능.
		해시 트리 : 각 노드에 그 자식 노드들에 보관된 값의 해시 or 자식 노드들의 레이블로부터 계산된 해시 값을 레이블로 붙여두는 트리.

![[Pasted image 20240525110654.png]]
* 머클 트리 생성 예시
	1. 키 공간을 버킷으로 나눔
	2. 버킷에 포함된 각각 키에 균등 분포 해시 함수 적용해 해시 값 계산
	3. 버킷별 해시값 계산 후 해당 해시 값을 레이블로 갖는 노드 생성
		부모 노드 𝐻( 𝐻(𝐴) + 𝐻(𝐵) )
	4. 자식 노드의 레이블로부터 새로운 해시값 계산해, 이진 트리 상향 구성

![[Pasted image 20240525110927.png]]
* 두 머클 트리 비교
	1. 루트 노드 해시 값 비교 
		일치하는 경우) 두 서버는 같은 데이터
	2. 다른 경우) 자식 노드 해시 값 비교
	3. 다른 데이터 갖는 버킷 찾으면 해당 버킷들만 동기화
➡️ 동기화해야 하는 데이터의 양은 실제로 존재하는 차이의 크기일 뿐. 서버 데이터 총량과 무관.
그러나, 실 서비스의 경우 버킷 하나의 크기가 꽤 큼. 

#### 데이터 센터 장애 처리
* 여러 데이터 센터에 다중화하는 것이 중요.

## 7.  키-값 저장소 시스템 아키텍처 다이어그램
![[Pasted image 20240525111629.png]]
* 위 아키텍처의 주된 기능
	* 클라이언트는 키-값 저장소가 제공하는 2가지 단순 API, get(key) 및 put(key) 와 통신
	* 중재자는 클라이언트에게 키-값 저장소에 대한 프락시 역할하는 노드
	* 노드는 안전 해시의 해시링 위에 분포
	* 노드를 자동 추가/삭제하도록, 시스템 완전 분산
	* 데이터는 여러 노드에 다중화
	* 모든 노드가 같은 책임을 지므로, SPOF(Single Piont of Failure) 존재하지 않음

완전 분산된 설계 채택했으므로, 모든 노드는 아래 기능 전부 지원해야함.
![[Pasted image 20240525112405.png]]

## 8. 쓰기 경로
* 쓰기 요청이 특정 노드에 전달되면 벌어지는 일 ex. 카산드라
![[Pasted image 20240525112517.png]]

1. 쓰기 요청이 커밋 로그 파일에 기록
2. 데이터가 메모리 캐시에 기록
3. 메모리 캐시가 가득차거나 특정 임계치 도달시, 데이터는 디스크에 있는 SSTable(Sorted-String Table : <키,값> 순서쌍을 정렬된 리스트 형태로 관리하는 테이블) 에 기록

#### 💡 카산드라(Cassandra) 
* 대용량의 데이터를 처리하기 위해 설계된 분산형 NoSQL 데이터베이스 관리 시스템

* 장점
	* 뛰어난 확장성
	* 높은 가용성
	* 유연한 데이터 모델
* 단점
	* 상대적으로 느린 쓰기 성능 : 데이터를 여러 노드에 복제해야하므로
	* 복잡한 쿼리 제한
	* 일관성 문제 : 일관성을 완벽하게 보장 못하며, 일관성 수준 설정 필요
* 예시
	* 대용량 데이터 처리 : 대규모 웹, 이벤트로그 등 처리
	* 실시간 분석 : 실시간으로 대량 데이터 분석 및 응답 
	* 분산 온라인 서비스 : 전세계 분산된 사용자 기반 서비스에서 데이터의 높은 가용성과 분산처리 필요시
 

## 9. 읽기 경로
![[Pasted image 20240525112755.png]]

1. 읽기 요청 받은 노드는 데이터가 메모리 캐시에 있는지부터 살펴봄
2. 데이터가 메모리에 없는 경우) 디스크에서 가져옴
	어느 SSTable에 찾는 키가 있는지 효율적으로 알기위해 블룸필터 사용
	* 블룸필터 
		원소가 집합에 속하는지 여부 검사하는 확률적 자료 구조

# 요약
* 분산 키-값 저장소가 가져야 하는 기능과 구현시 이용되는 기술
![[Pasted image 20240525114817.png]]